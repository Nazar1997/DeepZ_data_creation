{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from sparse_vector.sparse_vector import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y','M']]\n",
    "all_features = [i[:-4] for i in os.listdir('../data/hg19_features/sparse/') if i.endswith('.pkl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "\n",
    "def chrom_reader(chrom):\n",
    "    files = sorted([i for i in os.listdir(f'../data/hg19_dna/') if f\"{chrom}_\" in i])\n",
    "    return ''.join([load(f\"../data/hg19_dna/{file}\") for file in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7a761dcc1543228ca2b063f32c69dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decd5d5b0b764eacbafabdcf765bd9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1054), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 12.6 s, sys: 5.35 s, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load all the data\n",
    "DNA = {chrom:chrom_reader(chrom) for chrom in tqdm(chroms)}\n",
    "ZDNA = load('../data/hg19_zdna/sparse/ZDNA.pkl')\n",
    "ZHUNT = load('../data/hg19_zdna/sparse/ZHUNT.pkl')\n",
    "\n",
    "DNA_features = {feture: load(f'../data/hg19_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All DL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals):\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        dna_OHE = self.le.transform(list(self.dna_source[chrom][begin:end].upper()))\n",
    "        \n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((dna_OHE, np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]\n",
    "        \n",
    "        return (X, y)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49850/49850 [00:01<00:00, 34378.21it/s]\n",
      "100%|██████████| 48639/48639 [00:01<00:00, 33132.59it/s]\n",
      "100%|██████████| 39604/39604 [00:01<00:00, 35528.40it/s]\n",
      "100%|██████████| 38230/38230 [00:01<00:00, 32598.79it/s]\n",
      "100%|██████████| 36183/36183 [00:01<00:00, 35343.51it/s]\n",
      "100%|██████████| 34223/34223 [00:01<00:00, 32222.80it/s]\n",
      "100%|██████████| 31827/31827 [00:00<00:00, 34798.99it/s]\n",
      "100%|██████████| 29272/29272 [00:00<00:00, 35216.21it/s]\n",
      "100%|██████████| 28242/28242 [00:00<00:00, 35509.09it/s]\n",
      "100%|██████████| 27106/27106 [00:00<00:00, 30589.87it/s]\n",
      "100%|██████████| 27001/27001 [00:00<00:00, 35329.94it/s]\n",
      "100%|██████████| 26770/26770 [00:00<00:00, 35564.51it/s]\n",
      "100%|██████████| 23033/23033 [00:00<00:00, 35120.28it/s]\n",
      "100%|██████████| 21469/21469 [00:00<00:00, 35350.21it/s]\n",
      "100%|██████████| 20506/20506 [00:00<00:00, 35283.34it/s]\n",
      "100%|██████████| 18070/18070 [00:00<00:00, 29094.21it/s]\n",
      "100%|██████████| 16239/16239 [00:00<00:00, 35088.34it/s]\n",
      "100%|██████████| 15615/15615 [00:00<00:00, 35134.32it/s]\n",
      "100%|██████████| 11825/11825 [00:00<00:00, 35522.65it/s]\n",
      "100%|██████████| 12605/12605 [00:00<00:00, 33810.63it/s]\n",
      "100%|██████████| 9625/9625 [00:00<00:00, 35514.21it/s]\n",
      "100%|██████████| 10260/10260 [00:00<00:00, 35449.20it/s]\n",
      "100%|██████████| 31054/31054 [00:00<00:00, 35272.37it/s]\n",
      "100%|██████████| 11874/11874 [00:00<00:00, 35469.71it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7762.44it/s]\n"
     ]
    }
   ],
   "source": [
    "width = 5000\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "ints_in = []\n",
    "ints_out = []\n",
    "\n",
    "for chrm in chroms:\n",
    "    for st in trange(0, ZDNA[chrm].shape - width, width):\n",
    "        interval = [st, min(st + width, ZDNA[chrm].shape)]\n",
    "        if ZDNA[chrm][interval[0]: interval[1]].any():\n",
    "            ints_in.append([chrm, interval[0], interval[1]])\n",
    "        else:\n",
    "            ints_out.append([chrm, interval[0], interval[1]])\n",
    "\n",
    "ints_in = np.array(ints_in)\n",
    "ints_out = np.array(ints_out)[np.random.choice(range(len(ints_out)), size=len(ints_in) * 3, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized = np.vstack((ints_in, ints_out))\n",
    "equalized = [[inter[0], int(inter[1]), int(inter[2])] for inter in equalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_inds, test_inds = next(StratifiedKFold().split(equalized, [f\"{int(i < 400)}_{elem[0]}\"\n",
    "                                                                 for i, elem \n",
    "                                                                 in enumerate(equalized)]))\n",
    "\n",
    "train_intervals, test_intervals = [equalized[i] for i in train_inds], [equalized[i] for i in test_inds]\n",
    "\n",
    "train_dataset = Dataset(chroms, feature_names, \n",
    "                       DNA, DNA_features, \n",
    "                       ZDNA, train_intervals)\n",
    "\n",
    "test_dataset = Dataset(chroms, feature_names, \n",
    "                       DNA, DNA_features, \n",
    "                       ZDNA, test_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class DeepZ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(1058, 500, 2, bidirectional=True)\n",
    "        self.seq = nn.Sequential(\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(2 * 500, 500),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(500, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n) = self.rnn(x)\n",
    "        x = self.seq(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':20,\n",
    "          'num_workers':20,\n",
    "          'shuffle':True}\n",
    "\n",
    "loader_train = data.DataLoader(train_dataset, **params)\n",
    "loader_test = data.DataLoader(test_dataset, **params)\n",
    "    \n",
    "def loss_func(output, y_batch):\n",
    "    return torch.nn.NLLLoss()(torch.transpose(output, 2, 1), y_batch)\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    loss_log, acc_log, roc_auc_log, f1_log = [], [], [], []\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader_train:\n",
    "        X_batch, y_batch = X_batch.cuda(), y_batch.cuda().long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        pred = torch.argmax(output, dim=2)\n",
    "        y_pred = nn.Softmax(dim=1)(output)[:, :,1].detach().cpu().numpy().flatten()\n",
    "        if np.std(y_batch.cpu().numpy().flatten()) == 0:\n",
    "            roc_auc = 0\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_batch.cpu().numpy().flatten(),\n",
    "                                    nn.Softmax(dim=1)(output)[:, :,1].detach().cpu().numpy().flatten())\n",
    "        f1_log.append(f1_score(y_batch.cpu().numpy().flatten(),\n",
    "                         pred.cpu().numpy().flatten()))\n",
    "        roc_auc_log.append(roc_auc)\n",
    "        acc = torch.mean((pred == y_batch).float())\n",
    "        acc_log.append(acc.cpu().numpy())\n",
    "        loss = loss_func(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log, acc_log, roc_auc_log, f1_log\n",
    "\n",
    "def test(model):\n",
    "    loss_log, acc_log, roc_auc_log, f1_log = [], [], [], []\n",
    "    model.eval()\n",
    "    means = []\n",
    "    for X_batch, y_batch in loader_test:\n",
    "        X_batch, y_batch = X_batch.cuda(), y_batch.cuda().long()\n",
    "        output = model(X_batch)\n",
    "        means.append(y_batch.sum().cpu() / (1.0 * y_batch.shape[0]))\n",
    "        pred = torch.argmax(output, dim=2)\n",
    "        if np.std(y_batch.cpu().numpy().flatten()) == 0:\n",
    "            roc_auc = 0\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_batch.cpu().numpy().flatten(),\n",
    "                                    nn.Softmax(dim=1)(output)[:, :,1].detach().cpu().numpy().flatten())\n",
    "        f1_log.append(f1_score(y_batch.cpu().numpy().flatten(),\n",
    "                                  pred.cpu().numpy().flatten()))\n",
    "        roc_auc_log.append(roc_auc)\n",
    "        acc = torch.mean((pred == y_batch).float())\n",
    "        acc_log.append(acc.cpu().numpy())\n",
    "        loss = loss_func(output, y_batch)\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log, acc_log, roc_auc_log, f1_log\n",
    "\n",
    "def plot_history(train_history, valid_history, title, BatchSize, epoch_to_show=20):\n",
    "    plt.figure(figsize=(epoch_to_show, 4))\n",
    "    plt.title(title)    \n",
    "    \n",
    "    epoch_num = len(valid_history)\n",
    "    train_history = np.array([None] * (BatchSize * epoch_to_show) + train_history)\n",
    "    valid_history = np.array([None] * epoch_to_show + valid_history)\n",
    "    \n",
    "    plt.plot(np.linspace(epoch_num-epoch_to_show+1, epoch_num+1, (epoch_to_show+1)*BatchSize), \n",
    "             train_history[-(epoch_to_show+1)*BatchSize:], c='red', label='train')\n",
    "    plt.plot(np.linspace(epoch_num-epoch_to_show+1, epoch_num+1, epoch_to_show+1),\n",
    "                valid_history[-epoch_to_show-1:], c='green', label='test')\n",
    "    \n",
    "    plt.ylim((0, 1))\n",
    "    plt.yticks(np.linspace(0, 1, 11))\n",
    "    plt.xticks(np.arange(epoch_num-epoch_to_show+1, epoch_num+2), \n",
    "              np.arange(epoch_num-epoch_to_show, epoch_num+1).astype(int))\n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log, train_acc_log, train_auc_log, train_f1_log = [], [], [], []\n",
    "    val_log,   val_acc_log,   val_auc_log, val_f1_log   = [], [], [], []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {} of {}\".format(epoch + 1, n_epochs))\n",
    "        train_loss, train_acc, train_auc, train_f1 = train_epoch(model, opt)\n",
    "        val_loss, val_acc, val_auc, val_f1 = test(model)\n",
    "        \n",
    "        BatchSize = len(train_loss)\n",
    "        \n",
    "        train_log.extend(train_loss)\n",
    "        train_acc_log.extend(train_acc)\n",
    "        train_auc_log.extend(train_auc)\n",
    "        train_f1_log.extend(train_f1)\n",
    "\n",
    "        val_log.append(np.mean(val_loss))\n",
    "        val_acc_log.append(np.mean(val_acc))\n",
    "        val_auc_log.append(np.mean(val_auc))\n",
    "        val_f1_log.append(np.mean(val_f1))\n",
    "#         raise BaseException\n",
    "        \n",
    "        if (epoch % 1) == 0:\n",
    "            clear_output()\n",
    "            plot_history(train_log,     val_log,     'Loss',     BatchSize)    \n",
    "            plot_history(train_acc_log, val_acc_log, 'Accuracy', BatchSize)\n",
    "            plot_history(train_auc_log, val_auc_log, 'Auc',      BatchSize)\n",
    "            plot_history(train_f1_log, val_f1_log,   'F1',       BatchSize)\n",
    "            print(\"Epoch {} AUC = {:.2%}\".format(epoch+1, 1 - val_auc_log[-1]))\n",
    "            print(\"Epoch {} accuracy = {:.2%}\".format(epoch+1, 1 - val_acc_log[-1]))\n",
    "            \n",
    "            \n",
    "    print(\"Final AUC: {:.2}\".format(1 - val_auc_log[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 12749502\n"
     ]
    }
   ],
   "source": [
    "model = DeepZ()\n",
    "model = model.cuda()\n",
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.RMSprop(model.parameters(), lr=10**-4, weight_decay=10**-4)\n",
    "train(model, opt, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
